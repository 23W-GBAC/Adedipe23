# ADEDIPE DAMILOLA EMMANUEL                                                  
                             
# TOPIC 

## ARTIFICIAL INTELLIGENCE (AI)

*WHAT IS ARTIFICIAL INTELLIGENCE (AI)*

  *Artificial intelligence* is the simulation of human intelligence processes by machines, especially computer systems. Specific applications of AI include expert systems, natural language processing  speech recognition and machine vision 

It can also be defined as the development of computer systems that can perform tasks that typically require human intelligence. These tasks include learning, reasoning, problem-solving, understanding natural language, speech recognition, and visual perception. AI aims to create machines or software that can simulate human cognitive functions and adapt to new information.
 
## TYPES OF ARTIFICIAL INTELLIGENCE (AI)

According to my research there 3 major types of AI 
They include: 
### 1. Narrow or weak AI, 
### 2. General or strong AI, 
### 3. Artificial superintelligence.

## NARROW OR WEAK AI :

This type of AI is designed to perform a specific task or a narrow set of tasks. It operates within a predefined range and lacks the ability to generalize to different domains. Examples include voice assistants like Siri or Alexa, image recognition software, and recommendation algorithms

## GENERAL OR STRONG AI :

This is a more advanced form of AI that can understand, learn, and apply knowledge across a broad range of tasks similar to human intelligence. Achieving strong AI is a complex and challenging goal, and as of now, we have only developed narrow AI systems.

## ARTIFICAL SUPERINTELLIGENCE :

This represents a level of AI that surpasses human intelligence across all domains. It is an advanced form of AI that, if realized, could potentially exhibit creativity, emotional intelligence, and problem-solving abilities at a level beyond human capabilities.

OTHER TYPES OF ARTIFICIAL INTELLIGENCE (AI)
THEY INCLUDE :

## Machine Learning (ML): 
  This is a subset of AI where algorithms enable systems to learn and improve from experience. It includes techniques such as supervised learning, unsupervised learning, and reinforcement learning.

## Deep Learning: 
   This is a subfield of machine learning that involves neural networks with multiple layers (deep neural networks). Deep learning has been particularly successful in tasks such as image and speech recognition.

## Natural Language Processing (NLP): 
   NLP focuses on enabling machines to understand, interpret, and generate human language. Applications include chatbots, language translation, and sentiment analysis.

## Type 1: Reactive machines. 
   These AI systems have no memory and are task-specific. An example is Deep Blue, the IBM chess program that beat Garry Kasparov in the 1990s. Deep Blue can identify pieces on a chessboard and make predictions, but because it has no memory, it cannot use past experiences to inform future ones.

## Type 2: Limited memory. :
   These AI systems have memory, so they can use past experiences to inform future decisions. Some of the decision-making functions in self driving cars are designed this way.

## Type 3: Theory of mind. :
   Theory of mind is a psychology term. When applied to AI, it means the system would have the social intelligence to understand emotions. This type of AI will be able to infer human intentions and predict behavior, a necessary skill for AI systems to become integral members of human teams.

## Type 4: Self-awareness. 
 In this category, AI systems have a sense of self, which gives them consciousness. Machines with self-awareness understand their own current state. This type of AI does not yet exist. 

 #  HISTORY OF ARTIFICIAL INTELLIGENCE (AI)
   
*WHO IS THE FATHER OF AI* 
  John McCarthy is considered as the father of Artificial Intelligence. John McCarthy was an American computer scientist. The term "artificial intelligence" was coined by him. He is one of the founder of artificial intelligence, together with Alan Turing, Marvin Minsky, Allen Newell, and Herbert 

  In the first half of the 20th century, science fiction familiarized the world with the concept of artificially intelligent robots. It began with the “heartless” Tin man from the Wizard of Oz and continued with the humanoid robot that impersonated Maria in Metropolis. By the 1950s, we had a generation of scientists, mathematicians, and philosophers with the concept of artificial intelligence (or AI) culturally assimilated in their minds. One such person was Alan Turing, a young British polymath who explored the mathematical possibility of artificial intelligence. Turing suggested that humans use available information as well as reason in order to solve problems and make decisions, so why can’t machines do the same thing? This was the logical framework of his 1950 paper, computing machinery and intelligence in which he discussed how to build intelligent machines and how to test their intelligence.

*The history of Artificial Intelligence (AI) spans several decades and has evolved through multiple phases of research, development, and technological advancements. Here's a brief overview of key milestones in the history of AI:*

  ### 1940s - 1950s: Early Concepts and Theoretical Foundations
  * The concept of AI can be traced back to the 1940s and 1950s. Mathematician and logician Alan Turing proposed the Turing Test in 1950, suggesting that a machine could be considered intelligent if it could exhibit human-like behavior indistinguishable from that of a human
    * 1956: Birth of AI as a Field
    * The term "Artificial Intelligence" was coined during the Dartmouth Conference in 1956. The conference, organized by John McCarthy, Marvin Minsky, Nathaniel Rochester, and Claude Shannon, marked the official beginning of AI as a distinct field of study.

### 1950s - 1960s: Early AI Programs :

At the beginning of 1950, John Von Neumann and Alan Turing did not create the term AI but were the founding fathers of the technology behind it: they made the transition from computers to 19th century decimal logic (which thus dealt with values from 0 to 9) and machines to binary logic (which rely on Boolean algebra, dealing with more or less important chains of 0 or 1). The two researchers thus formalized the architecture of our contemporary computers and demonstrated that it was a universal machine, capable of executing what is programmed. Turing, on the other hand, raised the question of the possible intelligence of a machine for the first time in his famous 1950 article "Computing Machinery and Intelligence" and described a "game of imitation", where a human should be able to distinguish in a teletype dialogue whether he is talking to a man or a machine. However controversial this article may be (this "Turing test" does not appear to qualify for many experts), it will often be cited as being at the source of the questioning of the boundary between the human and the machine.Early AI research focused on symbolic reasoning and problem-solving. Programs like the Logic Theorist (1956) by Allen Newell and Herbert A. Simon and the General Problem Solver (1957) laid the groundwork for AI development.

### 1960s - 1970s: Rule-Based Systems and Expert Systems

   * AI researchers developed rule-based systems and expert systems, which encoded human knowledge in the form of rules. The Dendral project (1965) for chemical analysis and the Mycin system (1976) for medical diagnosis were notable examples.

### 1980s: Knowledge-Based Systems and Expert Systems Boom

   * The 1980s saw a surge in interest and investment in AI, particularly in expert systems. However, overhyped expectations and limitations in existing technology led to a period known as the "AI Winter," marked by reduced funding and enthusiasm.

### Late 1980s - 1990s: Machine Learning Renaissance

   * During the late 1980s and 1990s, there was a resurgence of interest in AI, fueled in part by advancements in machine learning. Neural networks, particularly backpropagation algorithms, gained attention. The field of machine learning began to thrive.

### 1997: Deep Blue vs. Garry Kasparov

   * IBM's Deep Blue, a chess-playing computer, defeated world chess champion Garry Kasparov in a landmark match in 1997. This event showcased the potential of AI in specialized tasks.

### 2000s: Rise of Data-Driven AI and Big Data

  * The 2000s witnessed a shift toward data-driven approaches in AI. The availability of large datasets and improvements in computing power contributed to advancements in machine learning and the emergence of Big Data analytics.Further advances in machine learning, deep learning, NLP, speech recognition and computer vision gave rise to products and services that have shaped the way we live today. These include the 2000 launch of Google's search engine and the 2001 launch of Amazon's recommendation engine. Netflix developed its recommendation system for movies, Facebook introduced its facial recognition system and Microsoft launched its speech recognition system for transcribing speech into text. IBM launched Watson and Google started its self-driving initiative, Waymo

 ### 2010s: Deep Learning Dominance and AI Applications

 * Deep learning, a subfield of machine learning involving neural networks with multiple layers, gained prominence in the 2010s. Breakthroughs in image and speech recognition, natural language processing, and autonomous systems fueled the application of AI in various industries.The decade between 2010 and 2020 saw a steady stream of AI developments. These include the launch of Apple's Siri and Amazon's Alexa voice assistants; IBM Watson's victories on Jeopardy; self-driving cars; the development of the first generative adversarial network; the launch of TensorFlow, Google's open source deep learning framework; the founding of research lab OpenAI, developers of the GPT-3 language model and Dall-E image generator; the defeat of world Go champion Lee Sedol by Google DeepMind's AlphaGo; and the implementation of AI-based systems that detect cancers with a high degree of accuracy.

 ### Present and Future: Continued Advancements and Ethical Considerations

 * AI continues to advance rapidly, with applications in healthcare, finance, autonomous vehicles, and more. Ethical considerations, transparency, and responsible AI development have become key areas of focus.
